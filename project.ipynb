{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinyuhou/Desktop/NUS PhD/Third Year/BDC6223 Proseminar/Bandi/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import langchain, langgraph, dspy, openai, chromadb, streamlit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.document_loaders import UnstructuredWordDocumentLoader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI\n",
    "import requests\n",
    "from langchain.agents import Tool\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "import arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "# Set your OpenAI API key on the module\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "# Create a chat completion using the GPT-3.5-turbo model\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello, world!\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print the content of the first message in the response\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Task 1: Summarize offline PDFs that contain some research abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yx/kgp01h8577s73p1b9nls0db80000gn/T/ipykernel_23855/1155951833.py:14: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings()\n",
      "/var/folders/yx/kgp01h8577s73p1b9nls0db80000gn/T/ipykernel_23855/1155951833.py:18: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(temperature=0)\n",
      "/var/folders/yx/kgp01h8577s73p1b9nls0db80000gn/T/ipykernel_23855/1155951833.py:23: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  answer = qa.run(query)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_7a49e_row0_col0, #T_7a49e_row0_col1, #T_7a49e_row1_col0, #T_7a49e_row1_col1, #T_7a49e_row2_col0, #T_7a49e_row2_col1, #T_7a49e_row3_col0, #T_7a49e_row3_col1, #T_7a49e_row4_col0, #T_7a49e_row4_col1 {\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_7a49e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_7a49e_level0_col0\" class=\"col_heading level0 col0\" >Title</th>\n",
       "      <th id=\"T_7a49e_level0_col1\" class=\"col_heading level0 col1\" >Summary and Insights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7a49e_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_7a49e_row0_col0\" class=\"data row0 col0\" >Bo Lin.pdf</td>\n",
       "      <td id=\"T_7a49e_row0_col1\" class=\"data row0 col1\" >The paper presents a machine learning approach to solving bilevel<br>programs with a large number of independent followers, including two-stage<br>stochastic programming. The approach involves using a sampled subset of<br>followers and a machine learning model to estimate the objective<br>values of unsampled followers. The paper also includes follower sampling<br>algorithms and a representation learning approach to improve the quality<br>of leader decisions. Through numerical studies and a real-world case<br>study, the paper shows that this approach can generate higher<br>quality leader decisions and lead to potential cost savings.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7a49e_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_7a49e_row1_col0\" class=\"data row1 col0\" >Jingren Liu.pdf</td>\n",
       "      <td id=\"T_7a49e_row1_col1\" class=\"data row1 col1\" >The papers discuss the use of off-policy evaluation and off-policy<br>optimization in feature-based offline pricing, where a firm needs to<br>make a one-shot pricing decision for a single product based<br>on feature information. The proposed methodology is a doubly robust<br>off-policy estimate for the revenue function, which minimizes mean squared<br>error in off-policy evaluation and has theoretical convergence guarantees for<br>off-policy optimization. The papers also highlight the benefits of imposing<br>a shape restriction on the demand estimation procedure.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7a49e_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_7a49e_row2_col0\" class=\"data row2 col0\" >Xin Tong.pdf</td>\n",
       "      <td id=\"T_7a49e_row2_col1\" class=\"data row2 col1\" >The papers examine the problem of finding a decomposition of<br>a probability measure into sub-measures to minimize loss functions. They<br>explore the structures of the optimal sub-measures and introduce algorithms<br>based on Wasserstein gradient flow, demonstrating their convergence. Numerical results<br>show the feasibility of these algorithms and provide additional insights.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7a49e_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_7a49e_row3_col0\" class=\"data row3 col0\" >Zhi Wang.pdf</td>\n",
       "      <td id=\"T_7a49e_row3_col1\" class=\"data row3 col1\" >The paper explores the effectiveness of using a single-hidden-layer neural<br>network to represent the mixed logit choice model. It shows<br>that the approximation error of the neural network is not<br>affected by the curse of dimensionality and overparameterization does not<br>lead to overfitting. The paper also proves that the noisy<br>gradient descent algorithm can find the global optimizer of the<br>entropic regularized non-convex parameter learning problem. Experiments on synthetic and<br>real datasets validate the algorithm's superior performance compared to existing<br>benchmarks. These findings highlight the potential of using over-parameterized neural<br>network representations and efficient training algorithms to effectively learn complex<br>choice models with strong statistical and optimality guarantees.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7a49e_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_7a49e_row4_col0\" class=\"data row4 col0\" >Zhiyi Li.pdf</td>\n",
       "      <td id=\"T_7a49e_row4_col1\" class=\"data row4 col1\" >The paper discusses the Robust Satisficing (RS) model, which is<br>a new approach to robust optimization that offers streamlined procedures<br>and robust generalization. The paper analyzes the theoretical properties of<br>the RS model, including deriving statistical guarantees and confidence intervals<br>for the optimal loss. The RS model is also extended<br>to handle distribution shifts and f-divergence, and numerical experiments show<br>its superiority over other models in small-sample regimes and under<br>distribution shifts. The RS model is also shown to be<br>less sensitive to hyperparameter tuning compared to other models, making<br>it a practical choice for robustness considerations.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x16d319010>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "pdf_files = [\"Bo Lin.pdf\", \"Jingren Liu.pdf\", \"Xin Tong.pdf\", \"Zhi Wang.pdf\", \"Zhiyi Li.pdf\"]\n",
    "\n",
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "\n",
    "for doc in pdf_files:\n",
    "    # Step 1: Load the PDF Document\n",
    "    pdf_path = doc\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Step 2: Create Embeddings and Build a Vector Store\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "    # Step 3: Set Up a RetrievalQA Chain\n",
    "    llm = OpenAI(temperature=0)\n",
    "    qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever())\n",
    "    \n",
    "    # Step 4: Query the PDFs\n",
    "    query = \"Summarize the methodologies and key insights from these papers.\"\n",
    "    answer = qa.run(query)\n",
    "    \n",
    "    # Append the result with Title and Summary/Insights only\n",
    "    results.append([doc, answer])\n",
    "\n",
    "# Create a DataFrame from the results with only two columns\n",
    "df_results = pd.DataFrame(results, columns=[\"Title\", \"Summary and Insights\"])\n",
    "\n",
    "# Function to format the answer into multiple lines\n",
    "def format_answer(answer, max_line_length=80):\n",
    "    words = answer.split()\n",
    "    formatted_answer = \"\"\n",
    "    current_line = \"\"\n",
    "    \n",
    "    for word in words:\n",
    "        if len(current_line) + len(word) + 1 <= max_line_length:\n",
    "            current_line += \" \" + word\n",
    "        else:\n",
    "            formatted_answer += current_line.strip() + \"\\n\"\n",
    "            current_line = word\n",
    "    \n",
    "    formatted_answer += current_line.strip()\n",
    "    return formatted_answer\n",
    "\n",
    "# Apply formatting to the \"Summary and Insights\" column\n",
    "df_results[\"Summary and Insights\"] = df_results[\"Summary and Insights\"].apply(format_answer)\n",
    "\n",
    "def wrap_text_with_br(text, words_per_line=10):\n",
    "    words = text.split()\n",
    "    lines = []\n",
    "    for i in range(0, len(words), words_per_line):\n",
    "        lines.append(\" \".join(words[i : i + words_per_line]))\n",
    "    # Join lines with <br> for HTML line breaks\n",
    "    return \"<br>\".join(lines)\n",
    "\n",
    "df_results[\"Summary and Insights\"] = df_results[\"Summary and Insights\"].apply(\n",
    "    lambda x: wrap_text_with_br(x, words_per_line=10)\n",
    ")\n",
    "\n",
    "# Tell Pandas to use 'pre-wrap' so it respects line breaks\n",
    "df_styled = df_results.style.set_properties(**{\n",
    "    'white-space': 'pre-wrap',\n",
    "    'word-wrap': 'break-word'\n",
    "})\n",
    "\n",
    "display(df_styled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Task 2: Find the document that is most relevant to indicated topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RetrievalQA Answer ===\n",
      "\n",
      "The paper \"Neural-Network Mixed Logit Choice Model: Statistical and Optimality Guarantees\" is most relevant to optimization. This is because the\n",
      "paper specifically explores the effectiveness of using a neural network to represent the mixed logit choice model and proves that\n",
      "the noisy gradient descent algorithm can find the global optimizer of the entropic regularized non-convex parameter learning problem. The paper\n",
      "also provides experiments that validate the algorithm's superior performance compared to existing benchmarks. The paper title is \"Neural-Network Mixed Logit\n",
      "Choice Model: Statistical and Optimality Guarantees\" and it provides a comprehensive analysis of the theoretical properties of the RS model.\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Load All PDFs and Add Metadata ---\n",
    "documents = []\n",
    "for pdf in pdf_files:\n",
    "    loader = PyPDFLoader(pdf)\n",
    "    docs = loader.load()\n",
    "    # Add the file name to the document metadata so we know which paper it came from.\n",
    "    for doc in docs:\n",
    "        doc.metadata[\"source\"] = pdf\n",
    "    documents.extend(docs)\n",
    "\n",
    "# --- Step 2: Create Embeddings and Build a Vector Store ---\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# --- Option A: Use a RetrievalQA Chain ---\n",
    "llm = OpenAI(temperature=0)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "# Here we ask the agent to identify the paper most relevant to a given topic.\n",
    "query = (\"Which paper is most relevant to optimization? \"\n",
    "         \"Tell us your reasoning and provide the paper title and a brief explanation.\")\n",
    "answer = qa_chain.run(query)\n",
    "\n",
    "# Helper function to wrap text every 20 words\n",
    "def wrap_text(text, words_per_line=20):\n",
    "    words = text.split()\n",
    "    lines = []\n",
    "    for i in range(0, len(words), words_per_line):\n",
    "        lines.append(\" \".join(words[i:i + words_per_line]))\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# Format the answer by wrapping after every 20 words\n",
    "formatted_answer = wrap_text(answer, words_per_line=20)\n",
    "\n",
    "print(\"\\n=== RetrievalQA Answer ===\\n\")\n",
    "print(formatted_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yx/kgp01h8577s73p1b9nls0db80000gn/T/ipykernel_23855/1787704383.py:16: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  summary_sentence = llm(summary_prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Most Relevant Paper ===\n",
      "\n",
      "Paper file: Bo Lin.pdf\n",
      "Summary sentence: A machine learning approach to solving large bilevel and two-stage stochastic programs improves decision-making in a cycling network design problem, resulting in higher quality leader decisions and potential cost savings.\n"
     ]
    }
   ],
   "source": [
    "# Define your topic query (e.g., \"machine learning\" or \"bike planning\")\n",
    "topic_query = \"bike planning\"\n",
    "\n",
    "# Perform a similarity search and retrieve the top document\n",
    "results = vectorstore.similarity_search(topic_query, k=1)\n",
    "if results:\n",
    "    top_doc = results[0]\n",
    "    \n",
    "    # Use the LLM to generate a one-sentence summary of the document.\n",
    "    # If the document is long, you might consider passing only a relevant excerpt.\n",
    "    summary_prompt = (\n",
    "        \"Please provide a one-sentence summary that captures the most important \"\n",
    "        \"insight from the following document text:\\n\\n\"\n",
    "        f\"{top_doc.page_content}\"\n",
    "    )\n",
    "    summary_sentence = llm(summary_prompt)\n",
    "    \n",
    "    print(\"\\n=== Most Relevant Paper ===\\n\")\n",
    "    print(\"Paper file:\", top_doc.metadata.get(\"source\", \"Unknown\"))\n",
    "    print(\"Summary sentence:\", summary_sentence.strip())\n",
    "else:\n",
    "    print(\"No relevant documents found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Task 3: Compare the documents and rank them by similarity scores that is calculate by how related they are with the indicated topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Similarity Scores and Summaries for Each Paper ===\n",
      "\n",
      "Paper file: Bo Lin.pdf\n",
      "Similarity Score (lower is more similar): 0.46843255\n",
      "Summary: A machine learning approach to solving large bilevel and two-stage stochastic programs improves decision-making in a cycling network design problem, resulting in higher quality leader decisions and potential cost savings.\n",
      "--------------------------------------------------------------------------------\n",
      "Paper file: Jingren Liu.pdf\n",
      "Similarity Score (lower is more similar): 0.58280146\n",
      "Summary: The document discusses the challenges and solutions for optimizing offline pricing decisions using historical data and off-policy evaluation and optimization, with a focus on the benefits of imposing shape restrictions on demand estimation.\n",
      "--------------------------------------------------------------------------------\n",
      "Paper file: Xin Tong.pdf\n",
      "Similarity Score (lower is more similar): 0.5898756\n",
      "Summary: \"Researchers explore optimal decomposition of probability measures for clustering and user grouping using Wasserstein gradient flow algorithms.\"\n",
      "--------------------------------------------------------------------------------\n",
      "Paper file: Zhi Wang.pdf\n",
      "Similarity Score (lower is more similar): 0.6254742\n",
      "Summary: This study shows that using a single-hidden-layer neural network to represent the mixed logit choice model can effectively learn complex choice models with strong statistical and optimality guarantees, without suffering from the curse of dimensionality or overfitting.\n",
      "--------------------------------------------------------------------------------\n",
      "Paper file: Zhiyi Li.pdf\n",
      "Similarity Score (lower is more similar): 0.6291983\n",
      "Summary: The Robust Satisficing (RS) model offers a more straightforward and practical approach to deriving statistical guarantees compared to the Distributionally Robust Optimization (DRO) model, and shows consistent performance in small-sample regimes and under distribution shifts.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Load All PDFs with Metadata ---\n",
    "documents = []\n",
    "for pdf in pdf_files:\n",
    "    loader = PyPDFLoader(pdf)\n",
    "    docs = loader.load()\n",
    "    for doc in docs:\n",
    "        # Tag each document with its source (filename)\n",
    "        doc.metadata[\"source\"] = pdf\n",
    "    documents.extend(docs)\n",
    "\n",
    "# --- Step 2: Create Embeddings and Build a Vector Store ---\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "# --- Step 3: Define Your Query and Retrieve Similarity Scores ---\n",
    "topic_query = \"bike planning\"\n",
    "\n",
    "# Retrieve all documents along with their similarity scores.\n",
    "results = vectorstore.similarity_search_with_score(topic_query, k=len(documents))\n",
    "\n",
    "# --- Step 4: Print the Similarity Scores and Summaries for Each Paper ---\n",
    "print(\"\\n=== Similarity Scores and Summaries for Each Paper ===\\n\")\n",
    "for doc, score in results:\n",
    "    # Generate a one-sentence summary using your LLM.\n",
    "    summary_prompt = (\n",
    "        \"Please provide a one-sentence summary that captures the most important \"\n",
    "        \"insight from the following document text:\\n\\n\" + doc.page_content\n",
    "    )\n",
    "    summary_sentence = llm(summary_prompt)\n",
    "    \n",
    "    print(\"Paper file:\", doc.metadata.get(\"source\", \"Unknown\"))\n",
    "    print(\"Similarity Score (lower is more similar):\", score)\n",
    "    print(\"Summary:\", summary_sentence.strip())\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test task 4: Search for papers that are relevant to indicated topics online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yx/kgp01h8577s73p1b9nls0db80000gn/T/ipykernel_23855/1871608725.py:69: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m I should use INFORMSJournalSearch to find relevant papers.\n",
      "Action: INFORMSJournalSearch\n",
      "Action Input: \"bike lanes planning\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mTitle: Planning Bike Lanes with Data: Ridership, Congestion, and Path Selection\n",
      "Venue: Management Sciences\n",
      "Abstract: Urban infrastructure is vital for sustainable cities. In recent years, municipal governments have invested heavily in the expansion of bike lane networks to meet growing demand, promote ridership, and reduce emissions. However, reallocating road capacity to cycling is often contentious because of the risk of amplifying traffic congestion. In this paper, we develop a method for planning bike lanes that accounts for ridership and congestion effects. We first present a procedure for estimating parameters of a traffic equilibrium model, which combines an inverse optimization method for predicting driving times with an instrumental variables method for estimating a commuter mode choice model. We then formulate a prescriptive model that selects paths in a road network for bike lane installation while endogenizing cycling demand and driving travel times. We conduct an empirical study on the City of Chicago that brings together several data sets that describe the urban environment—including the road and bike lane networks, vehicle flows, commuter mode choices, bike share trips, driving and cycling routes, demographic features, and points of interest—with the goal of estimating the impact of expanding Chicago’s bike lane network. We estimate that adding 25 miles of bike lanes as prescribed by our model can lift cycling ridership from 3.6% to 6.1%, with at most a 9.4% increase in driving times. We also find that three intuitive heuristics for bike lane planning can lead to lower ridership and worse congestion outcomes, highlighting the value of a holistic and data-driven approach to urban infrastructure planning. This paper was accepted by Karan Girotra, operations management. Funding: Funding: The authors acknowledge funding from the UCLA Anderson Easton Technology Management Center (Siddiq & Zhang) and the Natural Sciences and Engineering Research Council of Canada [Grant RGPIN-2022-04950] and TD-MDAL Research Grant from the Rotman School of Management (Liu). Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.00775 .\n",
      "URL: https://www.semanticscholar.org/paper/457b68681092baa73e7a5f92c27985a21890465d\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should repeat the INFORMSJournalSearch action for other relevant keywords.\n",
      "Action: INFORMSJournalSearch\n",
      "Action Input: \"bike lanes planning\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mTitle: Planning Bike Lanes with Data: Ridership, Congestion, and Path Selection\n",
      "Venue: Management Sciences\n",
      "Abstract: Urban infrastructure is vital for sustainable cities. In recent years, municipal governments have invested heavily in the expansion of bike lane networks to meet growing demand, promote ridership, and reduce emissions. However, reallocating road capacity to cycling is often contentious because of the risk of amplifying traffic congestion. In this paper, we develop a method for planning bike lanes that accounts for ridership and congestion effects. We first present a procedure for estimating parameters of a traffic equilibrium model, which combines an inverse optimization method for predicting driving times with an instrumental variables method for estimating a commuter mode choice model. We then formulate a prescriptive model that selects paths in a road network for bike lane installation while endogenizing cycling demand and driving travel times. We conduct an empirical study on the City of Chicago that brings together several data sets that describe the urban environment—including the road and bike lane networks, vehicle flows, commuter mode choices, bike share trips, driving and cycling routes, demographic features, and points of interest—with the goal of estimating the impact of expanding Chicago’s bike lane network. We estimate that adding 25 miles of bike lanes as prescribed by our model can lift cycling ridership from 3.6% to 6.1%, with at most a 9.4% increase in driving times. We also find that three intuitive heuristics for bike lane planning can lead to lower ridership and worse congestion outcomes, highlighting the value of a holistic and data-driven approach to urban infrastructure planning. This paper was accepted by Karan Girotra, operations management. Funding: Funding: The authors acknowledge funding from the UCLA Anderson Easton Technology Management Center (Siddiq & Zhang) and the Natural Sciences and Engineering Research Council of Canada [Grant RGPIN-2022-04950] and TD-MDAL Research Grant from the Rotman School of Management (Liu). Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.00775 .\n",
      "URL: https://www.semanticscholar.org/paper/457b68681092baa73e7a5f92c27985a21890465d\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should repeat the INFORMSJournalSearch action for other relevant keywords.\n",
      "Action: INFORMSJournalSearch\n",
      "Action Input: \"bike lanes planning\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mTitle: Planning Bike Lanes with Data: Ridership, Congestion, and Path Selection\n",
      "Venue: Management Sciences\n",
      "Abstract: Urban infrastructure is vital for sustainable cities. In recent years, municipal governments have invested heavily in the expansion of bike lane networks to meet growing demand, promote ridership, and reduce emissions. However, reallocating road capacity to cycling is often contentious because of the risk of amplifying traffic congestion. In this paper, we develop a method for planning bike lanes that accounts for ridership and congestion effects. We first present a procedure for estimating parameters of a traffic equilibrium model, which combines an inverse optimization method for predicting driving times with an instrumental variables method for estimating a commuter mode choice model. We then formulate a prescriptive model that selects paths in a road network for bike lane installation while endogenizing cycling demand and driving travel times. We conduct an empirical study on the City of Chicago that brings together several data sets that describe the urban environment—including the road and bike lane networks, vehicle flows, commuter mode choices, bike share trips, driving and cycling routes, demographic features, and points of interest—with the goal of estimating the impact of expanding Chicago’s bike lane network. We estimate that adding 25 miles of bike lanes as prescribed by our model can lift cycling ridership from 3.6% to 6.1%, with at most a 9.4% increase in driving times. We also find that three intuitive heuristics for bike lane planning can lead to lower ridership and worse congestion outcomes, highlighting the value of a holistic and data-driven approach to urban infrastructure planning. This paper was accepted by Karan Girotra, operations management. Funding: Funding: The authors acknowledge funding from the UCLA Anderson Easton Technology Management Center (Siddiq & Zhang) and the Natural Sciences and Engineering Research Council of Canada [Grant RGPIN-2022-04950] and TD-MDAL Research Grant from the Rotman School of Management (Liu). Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.00775 .\n",
      "URL: https://www.semanticscholar.org/paper/457b68681092baa73e7a5f92c27985a21890465d\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should repeat the INFORMSJournalSearch action for other relevant keywords.\n",
      "Action: INFORMSJournalSearch\n",
      "Action Input: \"bike lanes planning\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mError: Received status code 429 from Semantic Scholar API.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should try using different keywords or narrowing down my search.\n",
      "Action: INFORMSJournalSearch\n",
      "Action Input: \"bike lanes planning\" AND \"urban infrastructure\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mError: Received status code 429 from Semantic Scholar API.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should try using different keywords or narrowing down my search.\n",
      "Action: INFORMSJournalSearch\n",
      "Action Input: \"bike lanes planning\" AND \"urban infrastructure\" AND \"traffic congestion\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mError: Received status code 429 from Semantic Scholar API.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should try using different keywords or narrowing down my search.\n",
      "Action: INFORMSJournalSearch\n",
      "Action Input: \"bike lanes planning\" AND \"urban infrastructure\" AND \"traffic congestion\" AND \"data-driven approach\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mError: Received status code 429 from Semantic Scholar API.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should try using different keywords or narrowing down my search.\n",
      "Action: INFORMSJournalSearch\n",
      "Action Input: \"bike lanes planning\" AND \"urban infrastructure\" AND \"traffic congestion\" AND \"data-driven approach\" AND \"city planning\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mTitle: Planning Bike Lanes with Data: Ridership, Congestion, and Path Selection\n",
      "Venue: Management Sciences\n",
      "Abstract: Urban infrastructure is vital for sustainable cities. In recent years, municipal governments have invested heavily in the expansion of bike lane networks to meet growing demand, promote ridership, and reduce emissions. However, reallocating road capacity to cycling is often contentious because of the risk of amplifying traffic congestion. In this paper, we develop a method for planning bike lanes that accounts for ridership and congestion effects. We first present a procedure for estimating parameters of a traffic equilibrium model, which combines an inverse optimization method for predicting driving times with an instrumental variables method for estimating a commuter mode choice model. We then formulate a prescriptive model that selects paths in a road network for bike lane installation while endogenizing cycling demand and driving travel times. We conduct an empirical study on the City of Chicago that brings together several data sets that describe the urban environment—including the road and bike lane networks, vehicle flows, commuter mode choices, bike share trips, driving and cycling routes, demographic features, and points of interest—with the goal of estimating the impact of expanding Chicago’s bike lane network. We estimate that adding 25 miles of bike lanes as prescribed by our model can lift cycling ridership from 3.6% to 6.1%, with at most a 9.4% increase in driving times. We also find that three intuitive heuristics for bike lane planning can lead to lower ridership and worse congestion outcomes, highlighting the value of a holistic and data-driven approach to urban infrastructure planning. This paper was accepted by Karan Girotra, operations management. Funding: Funding: The authors acknowledge funding from the UCLA Anderson Easton Technology Management Center (Siddiq & Zhang) and the Natural Sciences and Engineering Research Council of Canada [Grant RGPIN-2022-04950] and TD-MDAL Research Grant from the Rotman School of Management (Liu). Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mnsc.2022.00775 .\n",
      "URL: https://www.semanticscholar.org/paper/457b68681092baa73e7a5f92c27985a21890465d\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: The paper \"Planning Bike Lanes with Data: Ridership, Congestion, and Path Selection\" was published in the Management Sciences journal and provides a data-driven approach to planning bike lanes in urban areas. It was found through the INFORMSJournalSearch using the keywords \"bike lanes planning\", \"urban infrastructure\", \"traffic congestion\", \"data-driven approach\", and \"city planning\".\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "=== INFORMS Journal Search Result ===\n",
      "\n",
      "The paper \"Planning Bike Lanes with Data: Ridership, Congestion, and Path Selection\" was published in the Management Sciences journal and provides a data-driven approach to planning bike lanes in urban areas. It was found through the INFORMSJournalSearch using the keywords \"bike lanes planning\", \"urban infrastructure\", \"traffic congestion\", \"data-driven approach\", and \"city planning\".\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Define a Custom Tool to Search INFORMS Journals ---\n",
    "def informs_journal_search(query: str, limit: int = 10) -> str:\n",
    "    \"\"\"\n",
    "    Searches Semantic Scholar for academic papers on the given query,\n",
    "    then filters the results for papers published in known INFORMS journals,\n",
    "    particularly in the domains of Operations Research, Management Science,\n",
    "    MSOM, and other INFORMS outlets.\n",
    "    \"\"\"\n",
    "    # Known INFORMS journals (adjust as needed)\n",
    "    informs_journals = [\n",
    "        \"Management Science\", \n",
    "        \"Operations Research\", \n",
    "        \"Manufacturing & Service Operations Management\",\n",
    "        \"INFORMS Journal on Computing\",\n",
    "        \"INFORMS Journal on Applied Analytics\",\n",
    "        \"Interfaces\",\n",
    "        \"Transportation Science\"\n",
    "    ]\n",
    "    \n",
    "    url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"limit\": limit,  # Retrieve a larger set of papers for filtering\n",
    "        \"fields\": \"title,abstract,url,venue\"\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        return f\"Error: Received status code {response.status_code} from Semantic Scholar API.\"\n",
    "    \n",
    "    data = response.json()\n",
    "    papers = data.get(\"data\", [])\n",
    "    if not papers:\n",
    "        return \"No papers found for this query.\"\n",
    "    \n",
    "    # Filter papers whose 'venue' matches any known INFORMS journals.\n",
    "    filtered_papers = []\n",
    "    for paper in papers:\n",
    "        venue = paper.get(\"venue\", \"\")\n",
    "        if any(journal.lower() in venue.lower() for journal in informs_journals):\n",
    "            filtered_papers.append(paper)\n",
    "    \n",
    "    if not filtered_papers:\n",
    "        return \"No INFORMS journal papers found for this query.\"\n",
    "    \n",
    "    results = []\n",
    "    for paper in filtered_papers:\n",
    "        title = paper.get(\"title\", \"No title\")\n",
    "        abstract = paper.get(\"abstract\", \"No abstract available\")\n",
    "        paper_url = paper.get(\"url\", \"No URL available\")\n",
    "        venue = paper.get(\"venue\", \"Unknown venue\")\n",
    "        results.append(\n",
    "            f\"Title: {title}\\nVenue: {venue}\\nAbstract: {abstract}\\nURL: {paper_url}\\n\"\n",
    "        )\n",
    "    \n",
    "    return \"\\n\".join(results)\n",
    "\n",
    "# Wrap the function as a LangChain Tool.\n",
    "informs_tool = Tool(\n",
    "    name=\"INFORMSJournalSearch\",\n",
    "    func=informs_journal_search,\n",
    "    description=(\"Searches for academic papers using Semantic Scholar and filters the results \"\n",
    "                 \"to only include papers published in INFORMS journals (e.g., Operations Research, \"\n",
    "                 \"Management Science, MSOM, and related outlets).\")\n",
    ")\n",
    "\n",
    "# --- Step 2: Initialize the LangChain Agent with the Custom Tool ---\n",
    "llm = OpenAI(temperature=0)\n",
    "agent = initialize_agent(\n",
    "    tools=[informs_tool],\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "# --- Step 3: Query the Agent ---\n",
    "# Example query: find INFORMS journal papers on operations research, management science, or MSOM related to bike lanes planning.\n",
    "topic_query = (\"Find academic papers on operations research, management science, and MSOM published in INFORMS journals \"\n",
    "               \"that are relevant to bike lanes planning. Please provide a summary of key findings and the venue of each paper.\")\n",
    "result = agent.run(topic_query)\n",
    "\n",
    "print(\"\\n=== INFORMS Journal Search Result ===\\n\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Task: Literature review bot\n",
    "This bot is operated by 3 agents. The first agent extracts keywords from a PDF document. The keywords would then be passed to the second agent who searches relevant papers online. The third agent will compare and contrast the PDF document and the papers found by the second agent and output a literature review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yx/kgp01h8577s73p1b9nls0db80000gn/T/ipykernel_23855/3595073938.py:20: LangChainDeprecationWarning: The method `BaseLLM.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  keywords = llm.predict(prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Agent 1: Extracted Keywords ===\n",
      "Machine Learning, Optimization, Cycling Network Design\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use the arXivPaperSearch tool to find academic papers related to these topics.\n",
      "Action: arXivPaperSearch\n",
      "Action Input: \"Machine Learning OR Optimization OR Cycling Network Design\"\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yx/kgp01h8577s73p1b9nls0db80000gn/T/ipykernel_23855/3595073938.py:38: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Observation: \u001b[36;1m\u001b[1;3mTitle: Machine Learning-Augmented Optimization of Large Bilevel and Two-stage Stochastic Programs: Application to Cycling Network Design\n",
      "Authors: Timothy C. Y. Chan, Bo Lin, Shoshanna Saxe\n",
      "Summary: A wide range of decision problems can be formulated as bilevel programs with\n",
      "independent followers, which as a special case include two-stage stochastic\n",
      "programs. These problems are notoriously difficult to solve especially when a\n",
      "large number of followers present. Motivated by a real-world cycling\n",
      "infrastructure planning application, we present a general approach to solving\n",
      "such problems. We propose an optimization model that explicitly considers a\n",
      "sampled subset of followers and exploits a machine learning model to estimate\n",
      "the objective values of unsampled followers. We prove bounds on the optimality\n",
      "gap of the generated leader decision as measured by the original objective\n",
      "function that considers the full follower set. We then develop follower\n",
      "sampling algorithms to tighten the bounds and a representation learning\n",
      "approach to learn follower features, which are used as inputs to the embedded\n",
      "machine learning model. Through numerical studies, we show that our approach\n",
      "generates leader decisions of higher quality compared to baselines. Finally, in\n",
      "collaboration with the City of Toronto, we perform a real-world case study in\n",
      "Toronto where we solve a cycling network design problem with over one million\n",
      "followers. Compared to the current practice, our approach improves Toronto's\n",
      "cycling accessibility by 19.2%, equivalent to $18M in potential cost savings.\n",
      "Our approach is being used to inform the cycling infrastructure planning in\n",
      "Toronto and outperforms the current practice by a large margin. It can be\n",
      "generalized to any decision problems that are formulated as bilevel programs\n",
      "with independent followers.\n",
      "Published: 2022-09-20\n",
      "URL: http://arxiv.org/abs/2209.09404v4\n",
      "\n",
      "Title: Design Automation for Efficient Deep Learning Computing\n",
      "Authors: Song Han, Han Cai, Ligeng Zhu, Ji Lin, Kuan Wang, Zhijian Liu, Yujun Lin\n",
      "Summary: Efficient deep learning computing requires algorithm and hardware co-design\n",
      "to enable specialization: we usually need to change the algorithm to reduce\n",
      "memory footprint and improve energy efficiency. However, the extra degree of\n",
      "freedom from the algorithm makes the design space much larger: it's not only\n",
      "about designing the hardware but also about how to tweak the algorithm to best\n",
      "fit the hardware. Human engineers can hardly exhaust the design space by\n",
      "heuristics. It's labor consuming and sub-optimal. We propose design automation\n",
      "techniques for efficient neural networks. We investigate automatically\n",
      "designing specialized fast models, auto channel pruning, and auto\n",
      "mixed-precision quantization. We demonstrate such learning-based, automated\n",
      "design achieves superior performance and efficiency than rule-based human\n",
      "design. Moreover, we shorten the design cycle by 200x than previous work, so\n",
      "that we can afford to design specialized neural network models for different\n",
      "hardware platforms.\n",
      "Published: 2019-04-24\n",
      "URL: http://arxiv.org/abs/1904.10616v1\n",
      "\n",
      "Title: PowerPlanningDL: Reliability-Aware Framework for On-Chip Power Grid Design using Deep Learning\n",
      "Authors: Sukanta Dey, Sukumar Nandi, Gaurav Trivedi\n",
      "Summary: With the increase in the complexity of chip designs, VLSI physical design has\n",
      "become a time-consuming task, which is an iterative design process. Power\n",
      "planning is that part of the floorplanning in VLSI physical design where power\n",
      "grid networks are designed in order to provide adequate power to all the\n",
      "underlying functional blocks. Power planning also requires multiple iterative\n",
      "steps to create the power grid network while satisfying the allowed worst-case\n",
      "IR drop and Electromigration (EM) margin. For the first time, this paper\n",
      "introduces Deep learning (DL)-based framework to approximately predict the\n",
      "initial design of the power grid network, considering different reliability\n",
      "constraints. The proposed framework reduces many iterative design steps and\n",
      "speeds up the total design cycle. Neural Network-based multi-target regression\n",
      "technique is used to create the DL model. Feature extraction is done, and the\n",
      "training dataset is generated from the floorplans of some of the power grid\n",
      "designs extracted from the IBM processor. The DL model is trained using the\n",
      "generated dataset. The proposed DL-based framework is validated using a new set\n",
      "of power grid specifications (obtained by perturbing the designs used in the\n",
      "training phase). The results show that the predicted power grid design is\n",
      "closer to the original design with minimal prediction error (~2%). The proposed\n",
      "DL-based approach also improves the design cycle time with a speedup of ~6X for\n",
      "standard power grid benchmarks.\n",
      "Published: 2020-05-04\n",
      "URL: http://arxiv.org/abs/2005.01386v2\n",
      "\n",
      "Title: On the Difficulty of Designing Processor Arrays for Deep Neural Networks\n",
      "Authors: Kevin Stehle, Günther Schindler, Holger Fröning\n",
      "Summary: Systolic arrays are a promising computing concept which is in particular\n",
      "inline with CMOS technology trends and linear algebra operations found in the\n",
      "processing of artificial neural networks. The recent success of such deep\n",
      "learning methods in a wide set of applications has led to a variety of models,\n",
      "which albeit conceptual similar as based on convolutions and fully-connected\n",
      "layers, in detail show a huge diversity in operations due to a large design\n",
      "space: An operand's dimension varies substantially since it depends on design\n",
      "principles such as receptive field size, number of features, striding, dilating\n",
      "and grouping of features. Last, recent networks extent previously plain\n",
      "feedforward models by various connectivity, such as in ResNet or DenseNet. The\n",
      "problem of choosing an optimal systolic array configuration cannot be solved\n",
      "analytically, thus instead methods and tools are required that facilitate a\n",
      "fast and accurate reasoning about optimality in terms of total cycles,\n",
      "utilization, and amount of data movements. In this work we introduce Camuy, a\n",
      "lightweight model of a weight-stationary systolic array for linear algebra\n",
      "operations that allows quick explorations of different configurations, such as\n",
      "systolic array dimensions and input/output bitwidths. Camuy aids accelerator\n",
      "designers in either finding optimal configurations for a particular network\n",
      "architecture or for robust performance across a variety of network\n",
      "architectures. It offers simple integration into existing machine learning tool\n",
      "stacks (e.g TensorFlow) through custom operators. We present an analysis of\n",
      "popular DNN models to illustrate how it can estimate required cycles, data\n",
      "movement costs, as well as systolic array utilization, and show how the\n",
      "progress in network architecture design impacts the efficiency of inference on\n",
      "accelerators based on systolic arrays.\n",
      "Published: 2020-06-24\n",
      "URL: http://arxiv.org/abs/2006.14008v1\n",
      "\n",
      "Title: Human-in-the-Loop Design Cycles -- A Process Framework that Integrates Design Sprints, Agile Processes, and Machine Learning with Humans\n",
      "Authors: Chaehan So\n",
      "Summary: Demands on more transparency of the backbox nature of machine learning models\n",
      "have led to the recent rise of human-in-the-loop in machine learning, i.e.\n",
      "processes that integrate humans in the training and application of machine\n",
      "learning models. The present work argues that this process requirement does not\n",
      "represent an obstacle but an opportunity to optimize the design process. Hence,\n",
      "this work proposes a new process framework, Human-in-the-learning-loop (HILL)\n",
      "Design Cycles - a design process that integrates the structural elements of\n",
      "agile and design thinking process, and controls the training of a machine\n",
      "learning model by the human in the loop. The HILL Design Cycles process\n",
      "replaces the qualitative user testing by a quantitative psychometric\n",
      "measurement instrument for design perception. The generated user feedback\n",
      "serves to train a machine learning model and to instruct the subsequent design\n",
      "cycle along four design dimensions (novelty, energy, simplicity, tool). Mapping\n",
      "the four-dimensional user feedback into user stories and priorities, the design\n",
      "sprint thus transforms the user feedback directly into the implementation\n",
      "process. The human in the loop is a quality engineer who scrutinizes the\n",
      "collected user feedback to prevents invalid data to enter machine learning\n",
      "model training.\n",
      "Published: 2020-02-29\n",
      "URL: http://arxiv.org/abs/2003.05268v1\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The arXivPaperSearch tool provided me with five academic papers related to Machine Learning, Optimization, and Cycling Network Design. These papers cover topics such as using machine learning to optimize large decision problems, design automation for efficient deep learning computing, reliability-aware framework for on-chip power grid design, designing processor arrays for deep neural networks, and a human-in-the-loop design process framework that integrates design sprints, agile processes, and machine learning with humans.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "=== Agent 2: arXiv Paper Search Results ===\n",
      "The arXivPaperSearch tool provided me with five academic papers related to Machine Learning, Optimization, and Cycling Network Design. These papers cover topics such as using machine learning to optimize large decision problems, design automation for efficient deep learning computing, reliability-aware framework for on-chip power grid design, designing processor arrays for deep neural networks, and a human-in-the-loop design process framework that integrates design sprints, agile processes, and machine learning with humans.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "=== Agent 3: Literature Review ===\n",
      "\n",
      "\n",
      "Introduction:\n",
      "Machine learning and optimization techniques have been widely used in various fields to solve complex decision-making problems. In recent years, there has been a growing interest in applying these techniques to the design of cycling networks. This literature review aims to compare and contrast our paper, which focuses on using machine learning and optimization for cycling network design, with five related academic papers retrieved from arXiv. The review will highlight similarities and differences between the papers and discuss overall trends and insights in this research area.\n",
      "\n",
      "Similarities:\n",
      "All six papers share a common focus on using machine learning and optimization techniques for solving complex problems. They also highlight the potential of these techniques in improving the design process and achieving better results. For instance, our paper and the paper by Li et al. both use machine learning to optimize large decision problems. Similarly, the paper by Zhang et al. and our paper both focus on design automation for efficient deep learning computing. This shows a growing trend in using machine learning and optimization for design automation in various fields.\n",
      "\n",
      "Differences:\n",
      "While all six papers share a common focus, there are also notable differences between them. One key difference is the specific application area of the techniques. For example, our paper and the paper by Li et al. focus on using machine learning and optimization for\n"
     ]
    }
   ],
   "source": [
    "############################\n",
    "# Agent 1: PDF Reader      #\n",
    "############################\n",
    "\n",
    "def extract_keywords_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Loads a PDF file from the given path, extracts its text, and outputs three keywords \n",
    "    that best summarize the paper, separated by commas.\n",
    "    \"\"\"\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    documents = loader.load()\n",
    "    full_text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "    \n",
    "    prompt = (\n",
    "        \"You are an academic expert. Read the following research paper content and extract \"\n",
    "        \"three keywords that best summarize the paper. Return only the three keywords separated by commas.\\n\\n\"\n",
    "        f\"{full_text}\\n\\n\"\n",
    "        \"Three keywords:\"\n",
    "    )\n",
    "    keywords = llm.predict(prompt)\n",
    "    return keywords.strip()\n",
    "\n",
    "############################\n",
    "# Agent 2: arXiv Paper Searcher (as a LangChain Tool) #\n",
    "############################\n",
    "\n",
    "def search_arxiv_papers(query: str, max_results: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Searches arXiv for academic papers relevant to the given query.\n",
    "    Returns a formatted string with the title, authors, summary, publication date, and URL of each paper.\n",
    "    \"\"\"\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        sort_by=arxiv.SortCriterion.Relevance\n",
    "    )\n",
    "    results = []\n",
    "    for result in search.results():\n",
    "        # Format the authors list\n",
    "        authors = \", \".join(author.name for author in result.authors)\n",
    "        results.append(\n",
    "            f\"Title: {result.title}\\n\"\n",
    "            f\"Authors: {authors}\\n\"\n",
    "            f\"Summary: {result.summary}\\n\"\n",
    "            f\"Published: {result.published.date()}\\n\"\n",
    "            f\"URL: {result.entry_id}\\n\"\n",
    "        )\n",
    "    if not results:\n",
    "        return \"No papers found for this query on arXiv.\"\n",
    "    return \"\\n\".join(results)\n",
    "\n",
    "# Wrap the arXiv search function as a LangChain Tool.\n",
    "arxiv_search_tool = Tool(\n",
    "    name=\"arXivPaperSearch\",\n",
    "    func=search_arxiv_papers,\n",
    "    description=(\n",
    "        \"Searches for academic papers on arXiv based on a given query. \"\n",
    "        \"The input should be a set of keywords (connected with OR) that capture the topics of interest.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- Initialize the LangChain agent for Agent 2 (arXiv Paper Searcher) ---\n",
    "agent_arxiv = initialize_agent(\n",
    "    tools=[arxiv_search_tool],\n",
    "    llm=OpenAI(temperature=0),\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "###############################\n",
    "# Agent 3: Literature Reviewer#\n",
    "###############################\n",
    "\n",
    "def generate_literature_review(paper_keywords: str, search_results: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a literature review by comparing the extracted keywords from our paper \n",
    "    with the search results from arXiv. The review should be between 300 and 500 words.\n",
    "    \"\"\"\n",
    "    prompt = (\n",
    "        \"You are an academic writing expert. Below are three keywords extracted from our paper and a set of related \"\n",
    "        \"academic papers retrieved from arXiv.\\n\\n\"\n",
    "        \"Paper Keywords:\\n\"\n",
    "        f\"{paper_keywords}\\n\\n\"\n",
    "        \"arXiv Search Results:\\n\"\n",
    "        f\"{search_results}\\n\\n\"\n",
    "        \"Based on the above, write a detailed literature review that compares and contrasts our paper with the related work, \"\n",
    "        \"highlights similarities and differences, and discusses overall trends and insights in the research area. \"\n",
    "        \"Please ensure that the literature review is more than 500 words.\"\n",
    "    )\n",
    "    review = llm.predict(prompt)\n",
    "    return review\n",
    "\n",
    "###########################################\n",
    "# Main Pipeline: Chain the Three Agents   #\n",
    "###########################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the LLM (used across agents)\n",
    "    llm = OpenAI(temperature=0)\n",
    "    \n",
    "    # Replace with the path to your PDF file\n",
    "    pdf_path = \"Bo Lin.pdf\"\n",
    "    \n",
    "    # Agent 1: Extract three keywords from the PDF\n",
    "    paper_keywords = extract_keywords_from_pdf(pdf_path)\n",
    "    print(\"=== Agent 1: Extracted Keywords ===\")\n",
    "    print(paper_keywords)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Convert the comma-separated keywords into an OR query.\n",
    "    # For example, if paper_keywords == \"bike, lanes, planning\", the query becomes \"bike OR lanes OR planning\".\n",
    "    keywords_list = [kw.strip() for kw in paper_keywords.split(\",\") if kw.strip()]\n",
    "    search_query = \" OR \".join(keywords_list)\n",
    "    \n",
    "    # Agent 2: Use the extracted keywords as a query to search for related academic papers on arXiv.\n",
    "    search_results = agent_arxiv.run(search_query)\n",
    "    print(\"=== Agent 2: arXiv Paper Search Results ===\")\n",
    "    print(search_results)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Agent 3: Generate a literature review combining the keywords and the arXiv search results.\n",
    "    literature_review = generate_literature_review(paper_keywords, search_results)\n",
    "    print(\"=== Agent 3: Literature Review ===\")\n",
    "    print(literature_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
